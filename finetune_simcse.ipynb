{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRZsz8bBP7d",
        "outputId": "8ace3845-7f5a-4613-b5b9-6b4b1fb7506e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/SimCSE-main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QhQptblaCSRe",
        "outputId": "1b9c1ec7-d2f0-4ac1-c37e-ff477699f6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.2.1\n",
            "  Downloading transformers-4.2.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 35.1 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.0\n",
            "  Downloading scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 305 kB/s \n",
            "\u001b[?25hCollecting prettytable==2.1.0\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.0.24-py3-none-any.whl (5.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.1 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 8)) (1.7.1)\n",
            "Collecting setuptools==49.3.0\n",
            "  Downloading setuptools-49.3.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 3)) (0.70.13)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 3)) (0.3.5.1)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 4)) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.0->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.0->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable==2.1.0->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (2022.6.15)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.7.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (1.8.2)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.18.2-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 48.1 MB/s \n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (2.11.3)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (3.2.2)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (7.1.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 67.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (2.1.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 70.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 248 kB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (21.4.0)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting starlette==0.19.1\n",
            "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (0.11.0)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 33.6 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 973 kB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 7)) (2.21)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1->-r /content/drive/MyDrive/SimCSE-main/requirements.txt (line 1)) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart, sacremoses\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=4e5f3255aaad6d4b07b3ec7f5346d78428bfa18f372e0f6c304db710a08130e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=e98874750d4e45ad5a7f2e04d023754ba2444ff6b75c6066bffd3edd674c5fb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=39a60005358d9c0d07bad43b7d96538b09e7c800603c9be164746d15650323e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ffmpy python-multipart sacremoses\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, multidict, markdown-it-py, h11, frozenlist, anyio, yarl, tqdm, starlette, pynacl, monotonic, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, backoff, asynctest, async-timeout, aiosignal, xxhash, uvicorn, tokenizers, scipy, sacremoses, python-multipart, pydub, pycryptodome, paramiko, pandas, orjson, httpx, fsspec, ffmpy, fastapi, analytics-python, aiohttp, transformers, setuptools, scikit-learn, prettytable, gradio, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.3.0\n",
            "    Uninstalling prettytable-3.3.0:\n",
            "      Successfully uninstalled prettytable-3.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.6.1 async-timeout-4.0.2 asynctest-0.13.0 backoff-1.10.0 bcrypt-3.2.2 cryptography-37.0.4 datasets-1.2.1 fastapi-0.78.0 ffmpy-0.3.0 frozenlist-1.3.0 fsspec-2022.5.0 gradio-3.0.24 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 multidict-6.0.2 orjson-3.7.7 pandas-1.1.5 paramiko-2.11.0 prettytable-2.1.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sacremoses-0.0.53 scikit-learn-0.24.0 scipy-1.5.4 setuptools-49.3.0 sniffio-1.2.0 starlette-0.19.1 tokenizers-0.9.4 tqdm-4.49.0 transformers-4.2.1 uc-micro-py-1.0.1 uvicorn-0.18.2 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dill==0.3.4\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.13 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\n",
            "Successfully installed dill-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1\n",
        "!pip install -r /content/drive/MyDrive/SimCSE-main/requirements.txt\n",
        "!pip install dill==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_sup_example.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5lRlDHfPR9Q",
        "outputId": "61ea851f-0197-47ab-ba17-952f30558a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/10/2022 13:03:30 - INFO - __main__ -   PyTorch: setting up devices\n",
            "model_ards12 ModelArguments(model_name_or_path='bert-base-uncased', model_type=None, config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, temp=0.05, pooler_type='cls', hard_negative_weight=0, do_mlm=False, mlm_weight=0.1, mlp_only_train=False)\n",
            "07/10/2022 13:03:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: True\n",
            "07/10/2022 13:03:30 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='/content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=128, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Jul10_13-03-30_ddff78e5caee', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=125, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset csv (./data/csv/default-72f77ffc1e7f0347/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n",
            "[INFO|configuration_utils.py:445] 2022-07-10 13:03:31,049 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:481] 2022-07-10 13:03:31,049 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:445] 2022-07-10 13:03:31,141 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:481] 2022-07-10 13:03:31,142 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1766] 2022-07-10 13:03:31,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1766] 2022-07-10 13:03:31,332 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "model_ards2 ModelArguments(model_name_or_path='bert-base-uncased', model_type=None, config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, temp=0.05, pooler_type='cls', hard_negative_weight=0, do_mlm=False, mlm_weight=0.1, mlp_only_train=False)\n",
            "[INFO|modeling_utils.py:1027] 2022-07-10 13:03:31,443 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1135] 2022-07-10 13:03:35,189 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1146] 2022-07-10 13:03:35,189 >> Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "ok model BertForCL(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): Pooler()\n",
            "  (mlp): MLPLayer(\n",
            "    (dense): Linear(in_features=768, out_features=256, bias=True)\n",
            "  )\n",
            "  (sim): Similarity(\n",
            "    (cos): CosineSimilarity()\n",
            "  )\n",
            ")\n",
            "Loading cached processed dataset at ./data/csv/default-72f77ffc1e7f0347/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/cache-3eeb6d430d806348.arrow\n",
            "[INFO|trainer.py:442] 2022-07-10 13:03:37,804 >> The following columns in the training set don't have a corresponding argument in `BertForCL.forward` and have been ignored: .\n",
            "[INFO|trainer.py:358] 2022-07-10 13:03:37,805 >> Using amp fp16 backend\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -   ***** Running training *****\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Num examples = 131438\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Num Epochs = 3\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Instantaneous batch size per device = 128\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Gradient Accumulation steps = 1\n",
            "07/10/2022 13:03:37 - INFO - simcse.trainers -     Total optimization steps = 3081\n",
            "  0% 0/3081 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "{'eval_stsb_spearman': 0.6893624555506679, 'eval_sickr_spearman': 0.647277305573511, 'eval_avg_sts': 0.6683198805620895, 'epoch': 0.12}\n",
            "  4% 125/3081 [02:29<44:40,  1.10it/s][INFO|trainer.py:1344] 2022-07-10 13:06:06,858 >> Saving model checkpoint to /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full\n",
            "[INFO|configuration_utils.py:300] 2022-07-10 13:06:06,866 >> Configuration saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/config.json\n",
            "[INFO|modeling_utils.py:817] 2022-07-10 13:06:08,637 >> Model weights saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/pytorch_model.bin\n",
            "{'eval_stsb_spearman': 0.6717101575045268, 'eval_sickr_spearman': 0.6571438544309282, 'eval_avg_sts': 0.6644270059677275, 'epoch': 0.24}\n",
            "{'eval_stsb_spearman': 0.6727689654663861, 'eval_sickr_spearman': 0.644681993038404, 'eval_avg_sts': 0.6587254792523951, 'epoch': 0.37}\n",
            "{'loss': 0.0373, 'learning_rate': 4.188575137942227e-05, 'epoch': 0.49}\n",
            "{'eval_stsb_spearman': 0.667462260741274, 'eval_sickr_spearman': 0.6434925028112892, 'eval_avg_sts': 0.6554773817762816, 'epoch': 0.49}\n",
            "{'eval_stsb_spearman': 0.6436661269027378, 'eval_sickr_spearman': 0.6339634204725763, 'eval_avg_sts': 0.638814773687657, 'epoch': 0.61}\n",
            "{'eval_stsb_spearman': 0.6608883843242942, 'eval_sickr_spearman': 0.6619241017704549, 'eval_avg_sts': 0.6614062430473746, 'epoch': 0.73}\n",
            "{'eval_stsb_spearman': 0.6683029107868604, 'eval_sickr_spearman': 0.6665759619758375, 'eval_avg_sts': 0.667439436381349, 'epoch': 0.85}\n",
            "{'loss': 0.0034, 'learning_rate': 3.377150275884453e-05, 'epoch': 0.97}\n",
            "{'eval_stsb_spearman': 0.6856100241387876, 'eval_sickr_spearman': 0.6825291561007965, 'eval_avg_sts': 0.684069590119792, 'epoch': 0.97}\n",
            "{'eval_stsb_spearman': 0.7100303043149725, 'eval_sickr_spearman': 0.7044573231824539, 'eval_avg_sts': 0.7072438137487131, 'epoch': 1.1}\n",
            " 37% 1125/3081 [22:08<29:47,  1.09it/s][INFO|trainer.py:1344] 2022-07-10 13:25:46,820 >> Saving model checkpoint to /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full\n",
            "[INFO|configuration_utils.py:300] 2022-07-10 13:25:46,826 >> Configuration saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/config.json\n",
            "[INFO|modeling_utils.py:817] 2022-07-10 13:25:48,520 >> Model weights saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/pytorch_model.bin\n",
            "{'eval_stsb_spearman': 0.6721656323103422, 'eval_sickr_spearman': 0.6956872283086661, 'eval_avg_sts': 0.6839264303095042, 'epoch': 1.22}\n",
            "{'eval_stsb_spearman': 0.6790507542073615, 'eval_sickr_spearman': 0.7040087607261235, 'eval_avg_sts': 0.6915297574667425, 'epoch': 1.34}\n",
            "{'loss': 0.0027, 'learning_rate': 2.5657254138266795e-05, 'epoch': 1.46}\n",
            "{'eval_stsb_spearman': 0.6932984422492008, 'eval_sickr_spearman': 0.7081988499222801, 'eval_avg_sts': 0.7007486460857404, 'epoch': 1.46}\n",
            "{'eval_stsb_spearman': 0.6930795364043758, 'eval_sickr_spearman': 0.7033778241776324, 'eval_avg_sts': 0.698228680291004, 'epoch': 1.58}\n",
            "{'eval_stsb_spearman': 0.6909721630658072, 'eval_sickr_spearman': 0.7020393414738565, 'eval_avg_sts': 0.6965057522698319, 'epoch': 1.7}\n",
            "{'eval_stsb_spearman': 0.7005823511840201, 'eval_sickr_spearman': 0.7074459624072377, 'eval_avg_sts': 0.7040141567956288, 'epoch': 1.83}\n",
            "{'loss': 0.0027, 'learning_rate': 1.7543005517689063e-05, 'epoch': 1.95}\n",
            "{'eval_stsb_spearman': 0.7100296441721687, 'eval_sickr_spearman': 0.7121343262497133, 'eval_avg_sts': 0.711081985210941, 'epoch': 1.95}\n",
            "{'eval_stsb_spearman': 0.7170870938761744, 'eval_sickr_spearman': 0.715269220178376, 'eval_avg_sts': 0.7161781570272752, 'epoch': 2.07}\n",
            " 69% 2125/3081 [41:50<15:26,  1.03it/s][INFO|trainer.py:1344] 2022-07-10 13:45:28,553 >> Saving model checkpoint to /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full\n",
            "[INFO|configuration_utils.py:300] 2022-07-10 13:45:28,560 >> Configuration saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/config.json\n",
            "[INFO|modeling_utils.py:817] 2022-07-10 13:45:30,354 >> Model weights saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/pytorch_model.bin\n",
            "{'eval_stsb_spearman': 0.709995940153712, 'eval_sickr_spearman': 0.7146750274224851, 'eval_avg_sts': 0.7123354837880986, 'epoch': 2.19}\n",
            "{'eval_stsb_spearman': 0.7120604356074215, 'eval_sickr_spearman': 0.7107930577420539, 'eval_avg_sts': 0.7114267466747377, 'epoch': 2.31}\n",
            "{'loss': 0.0032, 'learning_rate': 9.428756897111327e-06, 'epoch': 2.43}\n",
            "{'eval_stsb_spearman': 0.7066334734204264, 'eval_sickr_spearman': 0.7146050461076897, 'eval_avg_sts': 0.710619259764058, 'epoch': 2.43}\n",
            "{'eval_stsb_spearman': 0.7169040118994565, 'eval_sickr_spearman': 0.7177483935102056, 'eval_avg_sts': 0.717326202704831, 'epoch': 2.56}\n",
            "{'eval_stsb_spearman': 0.7142372687034738, 'eval_sickr_spearman': 0.714576899882247, 'eval_avg_sts': 0.7144070842928605, 'epoch': 2.68}\n",
            "{'eval_stsb_spearman': 0.7079013722426837, 'eval_sickr_spearman': 0.7139586435445355, 'eval_avg_sts': 0.7109300078936096, 'epoch': 2.8}\n",
            "{'loss': 0.0024, 'learning_rate': 1.314508276533593e-06, 'epoch': 2.92}\n",
            "{'eval_stsb_spearman': 0.7079334938905382, 'eval_sickr_spearman': 0.7136834733643966, 'eval_avg_sts': 0.7108084836274674, 'epoch': 2.92}\n",
            "100% 3081/3081 [1:00:38<00:00,  1.15it/s]07/10/2022 14:04:15 - INFO - simcse.trainers -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "07/10/2022 14:04:15 - INFO - simcse.trainers -   Loading best model from /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full (score: 0.7170870938761744).\n",
            "[INFO|configuration_utils.py:443] 2022-07-10 14:04:15,954 >> loading configuration file /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/config.json\n",
            "[INFO|configuration_utils.py:481] 2022-07-10 14:04:15,955 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForCL\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1025] 2022-07-10 14:04:15,956 >> loading weights file /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1143] 2022-07-10 14:04:19,658 >> All model checkpoint weights were used when initializing BertForCL.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2022-07-10 14:04:19,658 >> All the weights of BertForCL were initialized from the model checkpoint at /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForCL for predictions without further training.\n",
            "{'train_runtime': 3641.9918, 'train_samples_per_second': 0.846, 'epoch': 3.0}\n",
            "100% 3081/3081 [1:00:41<00:00,  1.18s/it]\n",
            "[INFO|trainer.py:1344] 2022-07-10 14:04:19,807 >> Saving model checkpoint to /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full\n",
            "[INFO|configuration_utils.py:300] 2022-07-10 14:04:19,815 >> Configuration saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/config.json\n",
            "[INFO|modeling_utils.py:817] 2022-07-10 14:04:21,516 >> Model weights saved in /content/drive/MyDrive/SimCSE-main/result/c5_ep10_bert_full/pytorch_model.bin\n",
            "07/10/2022 14:04:21 - INFO - __main__ -   ***** Train results *****\n",
            "07/10/2022 14:04:21 - INFO - __main__ -     epoch = 3.0\n",
            "07/10/2022 14:04:21 - INFO - __main__ -     train_runtime = 3641.9918\n",
            "07/10/2022 14:04:21 - INFO - __main__ -     train_samples_per_second = 0.846\n",
            "07/10/2022 14:04:21 - INFO - __main__ -   *** Evaluate ***\n",
            "07/10/2022 14:04:55 - INFO - root -   Generating sentence embeddings\n",
            "07/10/2022 14:05:09 - INFO - root -   Generated sentence embeddings\n",
            "07/10/2022 14:05:09 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation\n",
            "07/10/2022 14:05:24 - INFO - root -   Best param found at split 1: l2reg = 0.001                 with score 78.7\n",
            "07/10/2022 14:05:39 - INFO - root -   Best param found at split 2: l2reg = 0.0001                 with score 78.4\n",
            "07/10/2022 14:05:53 - INFO - root -   Best param found at split 3: l2reg = 0.0001                 with score 78.64\n",
            "07/10/2022 14:06:06 - INFO - root -   Best param found at split 4: l2reg = 0.0001                 with score 78.52\n",
            "07/10/2022 14:06:22 - INFO - root -   Best param found at split 5: l2reg = 1e-05                 with score 78.19\n",
            "07/10/2022 14:06:23 - INFO - root -   Generating sentence embeddings\n",
            "07/10/2022 14:06:27 - INFO - root -   Generated sentence embeddings\n",
            "07/10/2022 14:06:27 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation\n",
            "07/10/2022 14:06:32 - INFO - root -   Best param found at split 1: l2reg = 0.001                 with score 84.27\n",
            "07/10/2022 14:06:37 - INFO - root -   Best param found at split 2: l2reg = 1e-05                 with score 84.04\n",
            "07/10/2022 14:06:43 - INFO - root -   Best param found at split 3: l2reg = 0.001                 with score 84.44\n",
            "07/10/2022 14:06:48 - INFO - root -   Best param found at split 4: l2reg = 1e-05                 with score 83.94\n",
            "07/10/2022 14:06:53 - INFO - root -   Best param found at split 5: l2reg = 0.0001                 with score 83.38\n",
            "07/10/2022 14:06:54 - INFO - root -   Generating sentence embeddings\n",
            "07/10/2022 14:07:08 - INFO - root -   Generated sentence embeddings\n",
            "07/10/2022 14:07:08 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation\n",
            "07/10/2022 14:07:20 - INFO - root -   Best param found at split 1: l2reg = 0.0001                 with score 93.59\n",
            "07/10/2022 14:07:33 - INFO - root -   Best param found at split 2: l2reg = 0.001                 with score 93.84\n",
            "07/10/2022 14:07:47 - INFO - root -   Best param found at split 3: l2reg = 0.001                 with score 93.74\n",
            "07/10/2022 14:08:01 - INFO - root -   Best param found at split 4: l2reg = 0.001                 with score 93.66\n",
            "07/10/2022 14:08:16 - INFO - root -   Best param found at split 5: l2reg = 0.0001                 with score 93.61\n",
            "07/10/2022 14:08:17 - INFO - root -   Generating sentence embeddings\n",
            "07/10/2022 14:08:20 - INFO - root -   Generated sentence embeddings\n",
            "07/10/2022 14:08:20 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation\n",
            "07/10/2022 14:08:33 - INFO - root -   Best param found at split 1: l2reg = 1e-05                 with score 87.65\n",
            "07/10/2022 14:08:46 - INFO - root -   Best param found at split 2: l2reg = 1e-05                 with score 86.89\n",
            "07/10/2022 14:09:01 - INFO - root -   Best param found at split 3: l2reg = 1e-05                 with score 87.51\n",
            "07/10/2022 14:09:15 - INFO - root -   Best param found at split 4: l2reg = 0.0001                 with score 87.26\n",
            "07/10/2022 14:09:30 - INFO - root -   Best param found at split 5: l2reg = 0.001                 with score 87.58\n",
            "07/10/2022 14:09:32 - INFO - root -   Computing embedding for train\n",
            "07/10/2022 14:10:18 - INFO - root -   Computed train embeddings\n",
            "07/10/2022 14:10:18 - INFO - root -   Computing embedding for dev\n",
            "07/10/2022 14:10:19 - INFO - root -   Computed dev embeddings\n",
            "07/10/2022 14:10:19 - INFO - root -   Computing embedding for test\n",
            "07/10/2022 14:10:21 - INFO - root -   Computed test embeddings\n",
            "07/10/2022 14:10:21 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..\n",
            "07/10/2022 14:10:45 - INFO - root -   [('reg:1e-05', 83.14), ('reg:0.0001', 83.37), ('reg:0.001', 83.72), ('reg:0.01', 82.11)]\n",
            "07/10/2022 14:10:45 - INFO - root -   Validation : best param found is reg = 0.001 with score             83.72\n",
            "07/10/2022 14:10:45 - INFO - root -   Evaluating...\n",
            "07/10/2022 14:10:50 - INFO - root -   ***** Transfer task : TREC *****\n",
            "\n",
            "\n",
            "07/10/2022 14:10:54 - INFO - root -   Computed train embeddings\n",
            "07/10/2022 14:10:55 - INFO - root -   Computed test embeddings\n",
            "07/10/2022 14:10:55 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation\n",
            "07/10/2022 14:11:05 - INFO - root -   [('reg:1e-05', 79.37), ('reg:0.0001', 79.37), ('reg:0.001', 78.7), ('reg:0.01', 74.1)]\n",
            "07/10/2022 14:11:05 - INFO - root -   Cross-validation : best param found is reg = 1e-05             with score 79.37\n",
            "07/10/2022 14:11:05 - INFO - root -   Evaluating...\n",
            "07/10/2022 14:11:05 - INFO - root -   ***** Transfer task : MRPC *****\n",
            "\n",
            "\n",
            "07/10/2022 14:11:06 - INFO - root -   Computing embedding for train\n",
            "07/10/2022 14:11:16 - INFO - root -   Computed train embeddings\n",
            "07/10/2022 14:11:16 - INFO - root -   Computing embedding for test\n",
            "07/10/2022 14:11:20 - INFO - root -   Computed test embeddings\n",
            "07/10/2022 14:11:20 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with 5-fold cross-validation\n",
            "07/10/2022 14:11:26 - INFO - root -   [('reg:1e-05', 73.7), ('reg:0.0001', 73.75), ('reg:0.001', 73.9), ('reg:0.01', 73.31)]\n",
            "07/10/2022 14:11:26 - INFO - root -   Cross-validation : best param found is reg = 0.001             with score 73.9\n",
            "07/10/2022 14:11:26 - INFO - root -   Evaluating...\n",
            "07/10/2022 14:11:27 - INFO - __main__ -   ***** Eval results *****\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     epoch = 3.0\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_CR = 84.01\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_MPQA = 87.38\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_MR = 78.49\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_MRPC = 73.9\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_SST2 = 83.72\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_SUBJ = 93.69\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_TREC = 79.37\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_avg_sts = 0.7161781570272752\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_avg_transfer = 82.93714285714285\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_sickr_spearman = 0.715269220178376\n",
            "07/10/2022 14:11:27 - INFO - __main__ -     eval_stsb_spearman = 0.7170870938761744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simcse_to_huggingface.py --path result/compare/c5_ep10_bert_full"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJyWKV9aH72_",
        "outputId": "d8444427-4dc0-445d-a098-93debadb254b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimCSE checkpoint -> Huggingface checkpoint for result/compare/c5_ep10_bert_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  finetuned model produce embedding of test data\n",
        "#!pip install transformers==4.10.1\n",
        "from transformers import AutoTokenizer,AutoModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time, torch, json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def extract_all_samples(dataset):\n",
        "    samples = []\n",
        "    utterances = []\n",
        "    personas = []\n",
        "    for d in dataset:\n",
        "        ua = [u['A'][0] for u in d['dialogue']]\n",
        "        ub = [u['B'][0] for u in d['dialogue']]\n",
        "        pa = [p for _, p in d['pa'].items() if not pd.isnull(p) and len(p) > 0]  # a=\" \" len(a)=1\n",
        "        pb = [p for _, p in d['pb'].items() if not pd.isnull(p) and len(p) > 0]\n",
        "        samples.append((ua, pa))\n",
        "        samples.append((ub, pb))\n",
        "\n",
        "        utterances += ua + ub\n",
        "        personas += pa + pb\n",
        "\n",
        "\n",
        "    return samples, utterances, personas \n",
        "\n",
        "lm='bert'\n",
        "input='/content/drive/MyDrive/SimCSE-main/persona_linking_test.json'\n",
        "output='/content/drive/MyDrive/SimCSE-main/result/workable_models/c5_ep10_bert_full.bert'\n",
        "# 将原文本用bert预处理为bert embedding, 生成bert文件\n",
        "\n",
        "if lm == 'bert':\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/SimCSE-main/result/compare/c5_ep10_bert_full')\n",
        "    model = AutoModel.from_pretrained('/content/drive/MyDrive/SimCSE-main/result/compare/c5_ep10_bert_full')\n",
        "    \n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "\n",
        "with open(input, 'r') as f:\n",
        "    # data preparation\n",
        "    train_dataset = json.load(f)\n",
        "    samples_text, utterances, personas = extract_all_samples(train_dataset)\n",
        "  \n",
        "\n",
        "    print('dialogues[{}], utterances[{}], personas[{}]'.format(\n",
        "        len(samples_text) / 2, len(utterances), len(personas)))\n",
        "\n",
        "u_d = []\n",
        "p_d = []\n",
        "\n",
        "b_time = time.time()\n",
        "\n",
        "for i in tqdm(range(len(samples_text))):  #显示处理到第几条数据\n",
        "    #print('i',i)\n",
        "    u_s = samples_text[i][0]\n",
        "    p_s = samples_text[i][1]\n",
        "   \n",
        "\n",
        "    u_tmp = torch.stack([model(tokenizer(u_t,max_length=25,padding=True,truncation=True,return_tensors='pt').to(device),output_hidden_states=True, return_dict=True).pooler_output for u_t in u_s], dim = 0)\n",
        "    u_tmp = torch.stack([model(tokenizer.encode(p_t,max_length=25,padding=True,truncation=True,return_tensors='pt').to(device)).pooler_output for p_t in p_s], dim = 0)\n",
        "    \n",
        "    u_d.append(u_tmp.data) #某个人的话语和对应的个人信息描述\n",
        "    p_d.append(p_tmp.data)\n",
        "\n",
        "torch.save({'utterance': u_d, 'persona': p_d}, output)\n",
        "\n",
        "e_time = time.time()\n",
        "\n",
        "print('time: {}'.format(e_time - b_time))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "FXpqWLrXFLsO",
        "outputId": "a9cc62fa-94dd-41fc-b255-44c5ecdadccb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dffcb50d6677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/SimCSE-main/result/compare/c5_ep10_bert_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/SimCSE-main/result/compare/c5_ep10_bert_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             return MODEL_MAPPING[type(config)].from_pretrained(\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             )\n\u001b[1;32m    731\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 raise RuntimeError(\n\u001b[1;32m   1158\u001b[0m                     \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[0;32m-> 1159\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     )\n\u001b[1;32m   1161\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertModel:\n\tsize mismatch for pooler.dense.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for pooler.dense.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([768])."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "finetune_simcse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}